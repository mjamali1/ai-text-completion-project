{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project: AI-Powered Text Completion\n",
    "\n",
    "#### Part 1: Building the Application\n",
    "\n",
    "1. Choose an AI Provider: Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install API\n",
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# load model \n",
    "model_name = \"gpt2\"  \n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Develop the Text Completion App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(prompt, max_length, temperature, top_p):\n",
    "\n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return response[0][\"generated_text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run multiple times\n",
    "def main():\n",
    "    print(\"Hugging Face Text Completion App (type 'exit' to quit)\\n\")\n",
    "    \n",
    "    prompt = \" \"\n",
    "    \n",
    "    ask = input(\"Would you like to set max_length, top_p, and temperature? (Y/N) \")\n",
    "    if (ask.lower() == \"y\"):\n",
    "        max_length = int(input(\"Enter max length of output (max 1,024): \"))\n",
    "        while max_length > 200 or max_length <= 0:\n",
    "            max_length = int(input(\"Try again. Enter max length of output (max 200): \"))\n",
    "\n",
    "        top_p = float(input(\"Enter top_p value (0.0 to 1.0): \"))\n",
    "        while top_p < 0.0 or top_p > 1.0:\n",
    "            top_p = float(input(\"Try again. Enter top_p value (0.0 to 1.0): \"))\n",
    "\n",
    "        temperature = float(input(\"Enter temperature value (0.0 to 1.0): \"))\n",
    "        while temperature < 0.0 or temperature > 1.0:\n",
    "            temperature = float(input(\"Try again. Enter temperature value (0.0 to 1.0): \"))\n",
    "    else: \n",
    "        max_length = 100\n",
    "        top_p = 0.9\n",
    "        temperature = 0.7\n",
    "\n",
    "    while prompt != \"\":\n",
    "        prompt = input(\"Enter your prompt: \")\n",
    "        if prompt.lower() == \"exit\":\n",
    "            break\n",
    "        response = generate_completion(prompt, max_length, temperature, top_p)\n",
    "        print(f\"\\nResponse:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Text Completion App (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "who are romeo and juliette, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the best of all the people, and who are the\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "how to braid hair.\n",
      "\n",
      "\"I'm not going to be able to do that,\" she said. \"I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be able to do that. I'm not going to be\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "are whales blue whales, and the whales that live in the ocean.\n",
      "\n",
      "The researchers also found that the whales that live in the ocean are more likely to be in the middle of the ocean, and that the whales that live in the ocean are more likely to be in the middle of the ocean.\n",
      "\n",
      "\"We found that the whales that live in the ocean are more likely to be in the middle of the ocean, and that the whales that live in the ocean are more likely to be in the middle of the ocean,\" said study co-author Dr. Michael K. Koppel, a marine biologist at the University of California, San Diego.\n",
      "\n",
      "The researchers also found that the whales that live in the ocean are more likely to be in the middle of the ocean, and that the whales that live in the ocean are more likely to be in the middle of the ocean.\n",
      "\n",
      "\"We found that the whales that live in the ocean are more likely to be in the middle of the ocean, and that the whales that live in the ocean are more likely to be in the middle of the ocean,\" Koppel said.\n",
      "\n",
      "The researchers also found that the whales that live in the ocean are more likely to be in the middle of the ocean, and that\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "finish the sentence: I can't go to work because... I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going to work. I'm not going\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempting a different API: openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.87.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OAI_generate_completion(prompt, max_length, temperature, top_p):\n",
    "\n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return response[0][\"generated_text\"].strip()\n",
    "\n",
    "\n",
    "def OAI_main():\n",
    "    print(\"Hugging Face Text Completion App (type 'exit' to quit)\\n\")\n",
    "    \n",
    "    prompt = \" \"\n",
    "    \n",
    "    ask = input(\"Would you like to set max_length, top_p, and temperature? (Y/N) \")\n",
    "    if (ask.lower() == \"y\"):\n",
    "        max_length = int(input(\"Enter max length of output (max 1,024): \"))\n",
    "        while max_length > 200 or max_length <= 0:\n",
    "            max_length = int(input(\"Try again. Enter max length of output (max 200): \"))\n",
    "\n",
    "        top_p = float(input(\"Enter top_p value (0.0 to 1.0): \"))\n",
    "        while top_p < 0.0 or top_p > 1.0:\n",
    "            top_p = float(input(\"Try again. Enter top_p value (0.0 to 1.0): \"))\n",
    "\n",
    "        temperature = float(input(\"Enter temperature value (0.0 to 1.0): \"))\n",
    "        while temperature < 0.0 or temperature > 1.0:\n",
    "            temperature = float(input(\"Try again. Enter temperature value (0.0 to 1.0): \"))\n",
    "    else: \n",
    "        max_length = 100\n",
    "        top_p = 0.9\n",
    "        temperature = 0.7\n",
    "\n",
    "    while prompt != \"\":\n",
    "        prompt = input(\"Enter your prompt: \")\n",
    "        if prompt.lower() == \"exit\":\n",
    "            break\n",
    "        response = generate_completion(prompt, max_length, temperature, top_p)\n",
    "        print(\"Response:\\n\")\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Text Completion App (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "who is shakespeare's most famous actor) and a \"squeaky-clean\" person. And she's a good actor. She's not.\n",
      "\n",
      "You have to take it one step further and say that there are not any women who are not shamed. There are people who are shamed, but they're not shamed. There are people who are shamed and they're not shamed.\n",
      "\n",
      "You have to take it one step further and say that there are not any women who are not shamed. There are people who are shamed and they're not shamed.\n",
      "\n",
      "The fact that a person who is shamed is shamed for a certain reason is not a good thing.\n",
      "\n",
      "The fact that a person who is shamed is shamed for a certain reason is not a good thing.\n",
      "\n",
      "I'm sure you can see that.\n",
      "\n",
      "You're going to have to say something, but you're going to have to say something. You have to say something.\n",
      "\n",
      "If you're going to do that, then it's going to have to be in a way that will make you feel like you're being shamed.\n",
      "\n",
      "So you're going to have to be shamed and I'm sure you're going to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "what is christmas and what is christmas? I mean, I've been doing this for a year now, and I've never seen any of this stuff ever come up in my life. I mean, I've been doing this for a year now, and I've never seen any of this stuff ever come up in my life. I mean, I'm just not interested in it. I don't want to see it. I don't want to see it.\n",
      "\n",
      "So I'm here. I'm just trying to come to terms with my past.\n",
      "\n",
      "I don't want to see it. I don't want to see it.\n",
      "\n",
      "So if you're wondering, I have no idea what it is that you're doing right now, but I have to tell you that you have to keep up with it. I don't want to see you getting your ass kicked by the people that are going to make you go out of your way to be a part of a group that doesn't want you.\n",
      "\n",
      "I don't want to see you going out of your way to be a part of a group that doesn't want you.\n",
      "\n",
      "I don't want to see you being a part of a group that doesn't want you.\n",
      "\n",
      "I don't\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "finish this sentece: The duck was...very happy with it. I'm so happy that I was able to get it. I'm so happy to be able to do this again. I love it. I love it so much.\n",
      "\n",
      "The duck is also made from an extremely durable plastic and is very easy to clean. I would highly recommend this duck to anyone looking to clean up their kitchen. I don't have any complaints about the duck, but I do have a few issues with the plastic. I used the duck to clean up the sink. I use it to clean the kitchen sink. The plastic is extremely hard to clean and it has a tough surface. It's hard to clean easily, so I was very pleased with the quality of the plastic.\n",
      "\n",
      "I have had a couple of issues with the duck and it is very easy to clean. It is very easy to clean. I use it to clean the kitchen sink. The plastic is extremely hard to clean and it has a tough surface. It's hard to clean easily, so I was very pleased with the quality of the plastic. It was very easy to clean. The duck is also made from an extremely durable plastic. I have had a couple of issues with the duck and it is very easy to clean. It is very easy to\n"
     ]
    }
   ],
   "source": [
    "OAI_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
