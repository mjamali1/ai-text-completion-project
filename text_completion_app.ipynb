{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project: AI-Powered Text Completion\n",
    "\n",
    "#### Part 1: Building the Application\n",
    "\n",
    "1. Choose an AI Provider: Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install API\n",
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# load model \n",
    "model_name = \"gpt2\"  \n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set up text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Develop the Text Completion App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(prompt, max_length, temperature, top_p):\n",
    "\n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return response[0][\"generated_text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run multiple times\n",
    "def main():\n",
    "    print(\"Hugging Face Text Completion App (type 'exit' to quit)\\n\")\n",
    "    \n",
    "    prompt = \" \"\n",
    "    \n",
    "    ask = input(\"Would you like to set max_length, top_p, and temperature? (Y/N) \")\n",
    "    if (ask.lower() == \"y\"):\n",
    "        max_length = int(input(\"Enter max length of output (max 1,024): \"))\n",
    "        while max_length > 200 or max_length <= 0:\n",
    "            max_length = int(input(\"Try again. Enter max length of output (max 200): \"))\n",
    "\n",
    "        top_p = float(input(\"Enter top_p value (0.0 to 1.0): \"))\n",
    "        while top_p < 0.0 or top_p > 1.0:\n",
    "            top_p = float(input(\"Try again. Enter top_p value (0.0 to 1.0): \"))\n",
    "\n",
    "        temperature = float(input(\"Enter temperature value (0.0 to 1.0): \"))\n",
    "        while temperature < 0.0 or temperature > 1.0:\n",
    "            temperature = float(input(\"Try again. Enter temperature value (0.0 to 1.0): \"))\n",
    "    else: \n",
    "        max_length = 100\n",
    "        top_p = 0.9\n",
    "        temperature = 0.7\n",
    "\n",
    "    while prompt != \"\":\n",
    "        prompt = input(\"Enter your prompt: \")\n",
    "        if prompt.lower() == \"exit\":\n",
    "            break\n",
    "        response = generate_completion(prompt, max_length, temperature, top_p)\n",
    "        print(f\"\\nResponse:\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Text Completion App (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "finish this sentence: the big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big bear jumped over the...\n",
      "\n",
      "The big\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "tell me about hibernation.\n",
      "\n",
      "I'm not sure if you've heard of hibernation, but it's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while. It's a term that's been around for a while.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "explain mitosis to me.\n",
      "\n",
      "I'm not sure if I'm being honest or not. I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not sure if I'm being honest or not.\n",
      "\n",
      "I'm not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attempting a different API: openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.87.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OAI_generate_completion(prompt, max_length, temperature, top_p):\n",
    "\n",
    "    response = generator(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return response[0][\"generated_text\"].strip()\n",
    "\n",
    "\n",
    "def OAI_main():\n",
    "    print(\"Open AI Text Completion App (type 'exit' to quit)\\n\")\n",
    "    \n",
    "    prompt = \" \"\n",
    "    \n",
    "    ask = input(\"Would you like to set max_length, top_p, and temperature? (Y/N) \")\n",
    "    if (ask.lower() == \"y\"):\n",
    "        max_length = int(input(\"Enter max length of output (max 1,024): \"))\n",
    "        while max_length > 200 or max_length <= 0:\n",
    "            max_length = int(input(\"Try again. Enter max length of output (max 200): \"))\n",
    "\n",
    "        top_p = float(input(\"Enter top_p value (0.0 to 1.0): \"))\n",
    "        while top_p < 0.0 or top_p > 1.0:\n",
    "            top_p = float(input(\"Try again. Enter top_p value (0.0 to 1.0): \"))\n",
    "\n",
    "        temperature = float(input(\"Enter temperature value (0.0 to 1.0): \"))\n",
    "        while temperature < 0.0 or temperature > 1.0:\n",
    "            temperature = float(input(\"Try again. Enter temperature value (0.0 to 1.0): \"))\n",
    "    else: \n",
    "        max_length = 100\n",
    "        top_p = 0.9\n",
    "        temperature = 0.7\n",
    "\n",
    "    while prompt != \"\":\n",
    "        prompt = input(\"Enter your prompt: \")\n",
    "        if prompt.lower() == \"exit\":\n",
    "            break\n",
    "        response = generate_completion(prompt, max_length, temperature, top_p)\n",
    "        print(\"Response:\\n\")\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Text Completion App (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "Finish this sentence: The bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "Explain photosynthesis to me like I’m 5 years old. I can't remember. But it is quite possible I was born at a time when the sun was very hot. It was about a million years ago. It was very hot and very hot. But when I was 2, I was already a little bit older. That's why I can't remember. I was just a little girl and I was very young. But I can't remember the day I was born. I can't remember the day I was born. I can't remember my name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't remember my last name. I don't\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "Finish this sentence: The bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…the bear ate…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "How do I make a burger with the potatoes?\n",
      "\n",
      "You can make your own potato burgers from the following methods.\n",
      "\n",
      "How to make your own potato burgers\n",
      "\n",
      "1. Heat oil in a large skillet over medium heat. Add potatoes. Cook, stirring occasionally, until potatoes are tender. Remove from heat. Stir in potato flakes and add to potatoes. Add salt and pepper. Cook, stirring occasionally, until potatoes are tender. Remove from heat. Stir in potato flakes and add to potatoes. Add salt and pepper. Cook, stirring occasionally, until potatoes are tender. Remove from heat. Stir in potato flakes and add to potatoes. Add salt and pepper. Cook, stirring occasionally, until potatoes are tender. Remove from heat. Stir in potato flakes and add to potatoes. Add salt and pepper. Cook, stirring occasionally, until potatoes are tender. Remove from heat. Stir in potato flakes and add to potatoes. Add salt and pepper. Cook, stirring occasionally, until potatoes are tender. Remove from heat. Stir in potato flakes and add to potatoes. Add salt and pepper. Cook, stirring occasionally, until potatoes are tender. Remove from heat. Stir in potato flakes and add to potatoes. Add salt and pepper. Cook, stirring occasionally, until potatoes are tender. Remove from heat. Stir in\n"
     ]
    }
   ],
   "source": [
    "OAI_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
